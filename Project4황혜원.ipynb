{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project4황혜원.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcC2NGUt0KvJFNwEU36L2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EllinaHwang/Project/blob/main/Project4%ED%99%A9%ED%98%9C%EC%9B%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**내가 설정한 포지션에서 적합한 데이터셋 을 구한 뒤 그에 맞는 가설을 세우고 가설을 검증합니다.**\n",
        "\n",
        "\n",
        "**가설 검증을 위한 딥러닝 파이프라인을 구축합니다.**\n",
        "\n",
        "\n",
        "**구축한 파이프라인은 완벽하지 않아도 되며, 큰 가설의 일부(곁 가지)에 대한 검증을 목표로 해도 좋습니다.**\n",
        "\n",
        "\n",
        "**모자란 부분에 대해서는 이후 프로젝트를 통해서 연계해 나가도 좋으며 이런 부분은 '한계점 및 추후 발전 방향' 등을 통해 제시해 주어도 됩니다.**"
      ],
      "metadata": {
        "id": "d2zGZ56kVeGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 관심 데이터 선정**"
      ],
      "metadata": {
        "id": "kvZB-QdwV0wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"지금 무엇을 해야 할까?\"는 연구자에겐 가장 어려운 질문 중 하나입니다. \n",
        "\n",
        "그렇기에 석사과정 혹은 사원급에 해당하는 초기 연구자는 연구소(혹은 회사)에서 하던 연구방향에서 +@를 구현하는 것이 대부분입니다.\n",
        "\n",
        "하지만 우리는 아직 연구소(혹은 회사)가 없는 상태입니다. 이런 상황에서 연구 주제를 정해야 한다면 주제는 떠오르지 않고, 써보고 싶은 Method만 떠오르는 것이 일반적입니다.\n",
        "\n",
        "그렇기 때문에 이번 프로젝트에서는 내가 취직하고 싶은 회사와 그 회사의 연구/사업 내용을 알아보고 해당 회사에서 풀고자 하는 문제와 비슷한 데이터를 찾아봅니다.\n",
        "\n",
        "그리고 그에 맞는 기술력을 키워보는 것을 목표로 하는 것이 좋습니다.\n",
        "\n",
        "----\n",
        "\n",
        "*  현대자동차, 소카 등의 자율주행차 관련 업무를 목표로 하고 있다면, 관련 기술을 시험해볼만한 데이터셋이 있는 지 찾아보아야 한다.\n",
        "\n",
        "\n",
        "*  Kakao i(카카오) 나 Clova(네이버)와 같은 서비스를 목표로 한다면 음성 인식, 음성 합성에 사용할 수 있는 데이터 셋을 찾아볼 수 있습니다.\n",
        "\n",
        "\n",
        "*  네이버 웹툰 등을 목표로 한다면, GAN등의 생성모델과 보안(Fingerprint recognition) 관련 딥러닝 기술을 적용해 볼 수 있는 데이터 셋을 찾는 게 좋습니다.\n",
        "\n",
        "\n",
        "*  Airbnb와 같은 대고객 서비스를 만들고 싶다면 고객 데이터의 통계분석이 필요하며, 추천시스템을 적용해 볼 수 있는 데이터를 찾는 것이 좋습니다.\n"
      ],
      "metadata": {
        "id": "j-NDYPu5V87P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 데이터 선정 이유**"
      ],
      "metadata": {
        "id": "I2BnzW0yWUE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여러분이 선정한 데이터와 그 데이터를 가공하면서 얻은 지식과 경험을 \"어떤 회사에서 높이 살 수 있을까?\", \"어디 회사의 어느부분에 적용해 볼 수 있을까\"를 생각해서 기록하여 봅니다"
      ],
      "metadata": {
        "id": "d74Y2wsgWZX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 데이터를 이용한 가설 수립**"
      ],
      "metadata": {
        "id": "0bFD1MxhWb9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 선정함과 동시에 데이터를 통해서 내가 무엇을 해볼 수 있을 지 가설을 세우는 것이 중요합니다.\n",
        "\n",
        "첫 번째로는 **쓸모있는 가설, 즉 이유가 명확한 가설**이어야 합니다.\n",
        "\n",
        "\n",
        "데이터기반의 사고방식(Data-driven Thinking)에 대한 마인드셋을 Section 1과 2에서 배웠습니다.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "이번 프로젝트에서는 이를 심화시켜서 진짜 필요한 기술을 찾아봅시다.\n",
        "\n",
        "적어도 내 생각에는 정말 쓸모있다고 생각할 수 있는 스토리라인을 만들어보세요!"
      ],
      "metadata": {
        "id": "kVZxKMhcWjJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 데이터 전처리**"
      ],
      "metadata": {
        "id": "PFx8t_9ieHZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가설을 정했다면, 데이터의 가공을 시작해봅니다.\n",
        "바로 모델에 적용할 수 있는 데이터도 있겠지만 데이터를 직접 보며 전처리를 해보는 것을 추천드립니다.\n",
        "\n",
        "*  데이터 전처리 예시\n",
        "\n",
        "\n",
        "\n",
        ">> 1.  데이터의 정규화(Normalization)\n",
        "\n",
        ">> 2. 노이즈 및 이상치(Outlier) 제거\n",
        "\n",
        ">> 3. 타겟 레이블(Label or Ground Truth) 생성 혹은 선택 등"
      ],
      "metadata": {
        "id": "You6vEfTeI0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. 딥러닝 방식 적용**"
      ],
      "metadata": {
        "id": "3I6Q4Wd-emyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "적용에 앞서 내가 가진 문제를 굳이 딥러닝을 적용해야 하는 지 확인할 필요가 있습니다.\n",
        "\n",
        "신경망 첫 시간에 엄청 큰 검을 들고 스테이크를 썰던 이미지를 기억하시나요?\n",
        "\n",
        "딥러닝의 가장 큰 장점은 \"어려운 문제를 더 어렵게 풀지만, 그 결과가 끝내주게 좋다\"는 것인데요.\n",
        "\n",
        "만약 너무 쉬운 문제에 딥러닝을 적용한다거나 DL이 아닌 ML 방법론들보다 더 많은 리소스를 하는데도 성능이 낮으면 안되겠죠?"
      ],
      "metadata": {
        "id": "TZae1dEFeqHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Chance Level 이 넘는지 확인**"
      ],
      "metadata": {
        "id": "f3CKaTrsesC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이진 분류(Binary Classification)는 Chance level이 0.5(50%)인 문제입니다. \n",
        "\n",
        "말 그대로 하나로 찍는 머신이 이라도 약 50%정도는 달성한다는 뜻인데요. (데이터가 편향된 경우는 제외)\n",
        "\n",
        "여러분이 선택한 문제가 MNIST와 같은 10개 클래스를 가진 다중 분류라면 Chance level이 0.1(10%)이 되겠죠.\n",
        "\n",
        "---\n",
        "\n",
        "여러분이 선택한 문제에 딥러닝을 적용한 결과가 Chance level 보다 월등하게 좋은 성능을 기록하는 지를 체크해봅니다.\n",
        "\n",
        "그렇지 않다면 데이터를 다시 들여다 보거나 모델을 다시 뜯어보면서 내 가설이 틀렸을 수 있다는 것을 확인하여 봅니다."
      ],
      "metadata": {
        "id": "8cxMA-qteupy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 모델 검증(Validation)**"
      ],
      "metadata": {
        "id": "u1Q-E1Ace4-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 만들어서 어느정도 성능이 나왔다면, CV을 통해서 일반화될 가능성이 있는 지 확인해봅니다.\n",
        "\n",
        "K-Fold 교차 검증을 통해 일반화가 어느정도 되는지 알 수 있습니다.\n",
        "\n",
        "더불어 하이퍼파라미터를 변경하면서 최적화까지 해 볼 수 있겠습니다.\n",
        "\n",
        "물론 해당 과정에서 너무 많은 시간이 소요되지 않도록 잘 조정해야 합니다."
      ],
      "metadata": {
        "id": "dwhOvwJre7pQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. (Option) Requirements.txt 제작 및 재구현**"
      ],
      "metadata": {
        "id": "VWpGYoOzfAId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which python\n",
        "!python --version\n",
        "!echo $PYTHONPATH # /env/python\n",
        "%env PYTHONPATH=  #pythonpath 초기화\n",
        "!which conda# should return /usr/local/bin/conda\n",
        "!conda --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f_h9pef3B9",
        "outputId": "3c57e08e-480b-435d-f08e-f642828dd20b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.7.12\n",
            "\n",
            "env: PYTHONPATH=#pythonpath 초기화\n",
            "/bin/bash: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = \"\"\"\n",
        "gensim==3.8.1\n",
        "pyLDAvis==2.1.2\n",
        "spacy==2.2.3\n",
        "scikit-learn==0.23.1\n",
        "seaborn==0.11.0\n",
        "squarify==0.4.3\n",
        "tensorboard==2.4.0 \n",
        "tensorboard-plugin-wit==1.7.0 \n",
        "tensorflow==2.3.0 \n",
        "tensorflow-estimator==2.3.0\n",
        "keras==2.0.8\n",
        "ipykernel\n",
        "nltk\n",
        "pandas\n",
        "numpy\n",
        "scipy\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UQiBua7WgQXv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = \"\"\"text_file = open(\"requirements.txt\", \"w+\");text_file.write(p);text_file.close()\"\"\" \n",
        "\n",
        "exec(c)"
      ],
      "metadata": {
        "id": "lKiA5v_8gyrG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3lSiJ1Ug4YZ",
        "outputId": "9bd4a5e7-3171-4372-ee0d-fbd9a2f4297a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iBL7JKjXg4Ry",
        "outputId": "e78db598-aa9a-4c86-804c-04d993fc7d37"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting pyLDAvis==2.1.2\n",
            "  Using cached pyLDAvis-2.1.2.tar.gz (1.6 MB)\n",
            "Collecting spacy==2.2.3\n",
            "  Using cached spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "Collecting scikit-learn==0.23.1\n",
            "  Using cached scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting seaborn==0.11.0\n",
            "  Using cached seaborn-0.11.0-py3-none-any.whl (283 kB)\n",
            "Collecting squarify==0.4.3\n",
            "  Using cached squarify-0.4.3-py3-none-any.whl (4.3 kB)\n",
            "Collecting tensorboard==2.4.0\n",
            "  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
            "Collecting tensorboard-plugin-wit==1.7.0\n",
            "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
            "Collecting tensorflow==2.3.0\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 40 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting keras==2.0.8\n",
            "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[K     |████████████████████████████████| 276 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 2)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (0.37.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (2.7.3)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn==0.11.0->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 8)) (1.42.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 29.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 10)) (1.6.3)\n",
            "Collecting numpy>=1.11.3\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 10)) (3.3.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 10)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0->-r requirements.txt (line 10)) (1.13.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8->-r requirements.txt (line 12)) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 15)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 15)) (2018.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.8.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 8)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 8)) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 8)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.0->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn==0.11.0->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn==0.11.0->-r requirements.txt (line 6)) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn==0.11.0->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 8)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.0->-r requirements.txt (line 8)) (3.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3->-r requirements.txt (line 4)) (4.62.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r requirements.txt (line 13)) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (0.2.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 13)) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 13)) (4.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=4.0.0->ipykernel->-r requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 3)) (8.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 3)) (21.2.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97738 sha256=30ccb6a65befe1fb2bd91d2017fa2ff33e3dc4d47372a9b2ff6964fa89bf2944\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/fb/41/e32e5312da9f440d34c4eff0d2207b46dc9332a7b931ef1e89\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, tensorboard-plugin-wit, thinc, tensorflow-estimator, tensorboard, h5py, gast, funcy, tensorflow, squarify, spacy, seaborn, scikit-learn, pyLDAvis, keras, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorboard-plugin-wit\n",
            "    Found existing installation: tensorboard-plugin-wit 1.8.0\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.0:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.11.2\n",
            "    Uninstalling seaborn-0.11.2:\n",
            "      Successfully uninstalled seaborn-0.11.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.17 gast-0.3.3 gensim-3.8.1 h5py-2.10.0 keras-2.0.8 numpy-1.18.5 pyLDAvis-2.1.2 scikit-learn-0.23.1 seaborn-0.11.0 spacy-2.2.3 squarify-0.4.3 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 thinc-7.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 콘다 설치 방법1\n",
        "# # Miniconda 설치\n",
        "# %%bash\n",
        "\n",
        "# MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "# MINICONDA_PREFIX=/usr/local\n",
        "# wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "# chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "# ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\n",
        "\n",
        "# import sys\n",
        "# sys.path\n",
        "\n",
        "# !ls /usr/local/lib/python3.8/dist-packages\n",
        "\n",
        "# import sys\n",
        "# _ = (sys.path.append(\"/usr/local/lib/python3.8/site-packages\"))\n",
        "\n",
        "# sys.path\n",
        "# !conda install --channel conda-forge featuretools --yes\n",
        "\n",
        "# !ls\n",
        "\n",
        "# !conda env create -f /content/drive/MyDrive/RogerHeederer/ChatBot/KoGPT2-personachat-master/environment.yml\n",
        "\n",
        "# %%shell\n",
        "# $conda activate cm\n",
        "\n",
        "# %%shell\n",
        "# conda info --envs"
      ],
      "metadata": {
        "id": "6kux6N4ywZUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 콘다 설치 방법2\n",
        "# # 가상환경 만들기\n",
        "# !conda create -n envname python==3.8\n",
        "\n",
        "# # 먼저 가상환경 실행시키고 가상환경 확인\n",
        "# !source activate envname && conda env list\n",
        "\n",
        "# # 설치파일이 있다면 설치하기\n",
        "# !conda install --channel conda-forge --file requirements.txt --yes"
      ],
      "metadata": {
        "id": "A1BrHLsiwdRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 콘다 설치 방법3\n",
        "# !pip install -q condacolab\n",
        "\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "\n",
        "# import sys\n",
        "# sys.path\n",
        "\n",
        "# %%shell\n",
        "# python --version\n",
        "\n",
        "# import condacolab\n",
        "# condacolab.check()"
      ],
      "metadata": {
        "id": "mkThAaD_w0i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze > requirements.txt\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "JMQj5PJBOoW7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "언제까지 Colab만 쓰지는 않을 것입니다.\n",
        "\n",
        "여러분이 만든 딥러닝 모델을 다시 사용할 수 있도록 저장하고 새로운 환경에서도 바로 동작할 수 있도록 Requirements.txt를 만들어봅니다.\n",
        "\n",
        "만든 Requirements.txt 를 이용하여 가상환경 혹은 독립된 PC에서 같은 프로젝트를 진행하여 봅니다."
      ],
      "metadata": {
        "id": "vclxy9yBe-Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **제출물**\n",
        "\n",
        "아래 발표물을 압축하여 AI_(기수)_(이름).zip 형태로 제출합니다. Ex) AI_00_홍길동.zip\n",
        "\n",
        "발표 영상 (5분 이상 10분 이내로 작성합니다\u001d.)\n",
        "코드가 포함된 .ipynb 파일\n",
        "(제작하였을 경우) PPT 혹은 Keynote 파일\n"
      ],
      "metadata": {
        "id": "ETsbDcjEfFZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **프로젝트 채점 기준**"
      ],
      "metadata": {
        "id": "_cFICRbtfK8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. \"여러분이 풀고자 하는 문제를 잘 설정하였는가?\"**\n",
        "\n",
        "여러분이 선택한 데이터셋을 사용하여 어떤 문제를 풀고자 하는 지를 영상에서 잘 설명해주세요.\n",
        "\n",
        "\n",
        "단순히 익숙하다는 이유로 MNIST, cifar100 데이터를 사용하면 안되겠죠?\n",
        "\n",
        "\n",
        "여러분만의 문제와 데이터셋을 마련하는 것이 이번 프로젝트의 첫 번째 목표입니다!"
      ],
      "metadata": {
        "id": "NWE_KIWdfOIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. \"문제를 풀기 위한 모델 선택을 알맞게 설정하였는가?\"**\n",
        "\n",
        "\n",
        "여러분이 선택한 모델에 대한 구조를 설명할 필요는 없으며 해당 모델을 왜 선택하였는 지에 대해서 설명해주세요.\n",
        "\n",
        "\n",
        "예를 들어, 이미지 분류 문제에 단순한 LSTM을 적용하면 안 될 겁니다.\n",
        "\n",
        "\n",
        "풀고자 하는 문제와 알맞는 모델을 잘 선택하는 것이 이번 프로젝트의 두 번째 목표입니다!\n"
      ],
      "metadata": {
        "id": "Vo--a6PnfYol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. \"모델 학습을 제대로 진행하였는가?\"**\n",
        "\n",
        "\n",
        "데이터를 어떻게 전처리하였고 모델에 입력하였는 지를 설명해주세요.\n",
        "\n",
        "\n",
        "모델의 성능이 좋아야 할 필요는 없겠지만 모델 학습은 진행되어야 합니다."
      ],
      "metadata": {
        "id": "ZekcM_tEffF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 완성하지 못한 부분에 대하여 \"한계점과 추후 해결 방안을 알맞게 작성하였는가?\"**\n",
        "\n",
        "\n",
        "프로젝트 기간 동안 제시한 문제에 대해서 만족할 만한 성능을 얻지 못할 수도 있습니다.\n",
        "\n",
        "\n",
        "여러분이 만족할 만한 결과를 위해서 어떤 방향으로 프로젝트를 발전시켜 나가야 할지 구체적으로 설명해주세요."
      ],
      "metadata": {
        "id": "weLV9CZ_fkEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 영상 길이는 5분 이상 10분 이내로 작성합니다.**\n",
        "\n",
        " 너무 길어지거나 너무 짧아지지 않도록 해주세요!"
      ],
      "metadata": {
        "id": "I3b9s5oJfo1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NqRaazydfKqb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4RGKhpFkV8bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l1oxpjlkV3cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23C2WmP6Vaap"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}