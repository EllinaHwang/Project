{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project4황혜원.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnYp3E5XvwbzvQo4aKrnmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EllinaHwang/Project/blob/main/Project4%ED%99%A9%ED%98%9C%EC%9B%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**내가 설정한 포지션에서 적합한 데이터셋 을 구한 뒤 그에 맞는 가설을 세우고 가설을 검증합니다.**\n",
        "\n",
        "\n",
        "**가설 검증을 위한 딥러닝 파이프라인을 구축합니다.**\n",
        "\n",
        "\n",
        "**구축한 파이프라인은 완벽하지 않아도 되며, 큰 가설의 일부(곁 가지)에 대한 검증을 목표로 해도 좋습니다.**\n",
        "\n",
        "\n",
        "**모자란 부분에 대해서는 이후 프로젝트를 통해서 연계해 나가도 좋으며 이런 부분은 '한계점 및 추후 발전 방향' 등을 통해 제시해 주어도 됩니다.**"
      ],
      "metadata": {
        "id": "d2zGZ56kVeGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 관심 데이터 선정**"
      ],
      "metadata": {
        "id": "kvZB-QdwV0wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"지금 무엇을 해야 할까?\"는 연구자에겐 가장 어려운 질문 중 하나입니다. \n",
        "\n",
        "그렇기에 석사과정 혹은 사원급에 해당하는 초기 연구자는 연구소(혹은 회사)에서 하던 연구방향에서 +@를 구현하는 것이 대부분입니다.\n",
        "\n",
        "하지만 우리는 아직 연구소(혹은 회사)가 없는 상태입니다. 이런 상황에서 연구 주제를 정해야 한다면 주제는 떠오르지 않고, 써보고 싶은 Method만 떠오르는 것이 일반적입니다.\n",
        "\n",
        "그렇기 때문에 이번 프로젝트에서는 내가 취직하고 싶은 회사와 그 회사의 연구/사업 내용을 알아보고 해당 회사에서 풀고자 하는 문제와 비슷한 데이터를 찾아봅니다.\n",
        "\n",
        "그리고 그에 맞는 기술력을 키워보는 것을 목표로 하는 것이 좋습니다.\n",
        "\n",
        "----\n",
        "\n",
        "*  현대자동차, 소카 등의 자율주행차 관련 업무를 목표로 하고 있다면, 관련 기술을 시험해볼만한 데이터셋이 있는 지 찾아보아야 한다.\n",
        "\n",
        "\n",
        "*  Kakao i(카카오) 나 Clova(네이버)와 같은 서비스를 목표로 한다면 음성 인식, 음성 합성에 사용할 수 있는 데이터 셋을 찾아볼 수 있습니다.\n",
        "\n",
        "\n",
        "*  네이버 웹툰 등을 목표로 한다면, GAN등의 생성모델과 보안(Fingerprint recognition) 관련 딥러닝 기술을 적용해 볼 수 있는 데이터 셋을 찾는 게 좋습니다.\n",
        "\n",
        "\n",
        "*  Airbnb와 같은 대고객 서비스를 만들고 싶다면 고객 데이터의 통계분석이 필요하며, 추천시스템을 적용해 볼 수 있는 데이터를 찾는 것이 좋습니다.\n"
      ],
      "metadata": {
        "id": "j-NDYPu5V87P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 데이터 선정 이유**"
      ],
      "metadata": {
        "id": "I2BnzW0yWUE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여러분이 선정한 데이터와 그 데이터를 가공하면서 얻은 지식과 경험을 \"어떤 회사에서 높이 살 수 있을까?\", \"어디 회사의 어느부분에 적용해 볼 수 있을까\"를 생각해서 기록하여 봅니다"
      ],
      "metadata": {
        "id": "d74Y2wsgWZX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 데이터를 이용한 가설 수립**"
      ],
      "metadata": {
        "id": "0bFD1MxhWb9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 선정함과 동시에 데이터를 통해서 내가 무엇을 해볼 수 있을 지 가설을 세우는 것이 중요합니다.\n",
        "\n",
        "첫 번째로는 **쓸모있는 가설, 즉 이유가 명확한 가설**이어야 합니다.\n",
        "\n",
        "\n",
        "데이터기반의 사고방식(Data-driven Thinking)에 대한 마인드셋을 Section 1과 2에서 배웠습니다.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "이번 프로젝트에서는 이를 심화시켜서 진짜 필요한 기술을 찾아봅시다.\n",
        "\n",
        "적어도 내 생각에는 정말 쓸모있다고 생각할 수 있는 스토리라인을 만들어보세요!"
      ],
      "metadata": {
        "id": "kVZxKMhcWjJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 데이터 전처리**"
      ],
      "metadata": {
        "id": "PFx8t_9ieHZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가설을 정했다면, 데이터의 가공을 시작해봅니다.\n",
        "바로 모델에 적용할 수 있는 데이터도 있겠지만 데이터를 직접 보며 전처리를 해보는 것을 추천드립니다.\n",
        "\n",
        "*  데이터 전처리 예시\n",
        "\n",
        "\n",
        "\n",
        ">> 1.  데이터의 정규화(Normalization)\n",
        "\n",
        ">> 2. 노이즈 및 이상치(Outlier) 제거\n",
        "\n",
        ">> 3. 타겟 레이블(Label or Ground Truth) 생성 혹은 선택 등"
      ],
      "metadata": {
        "id": "You6vEfTeI0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. 딥러닝 방식 적용**"
      ],
      "metadata": {
        "id": "3I6Q4Wd-emyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "적용에 앞서 내가 가진 문제를 굳이 딥러닝을 적용해야 하는 지 확인할 필요가 있습니다.\n",
        "\n",
        "신경망 첫 시간에 엄청 큰 검을 들고 스테이크를 썰던 이미지를 기억하시나요?\n",
        "\n",
        "딥러닝의 가장 큰 장점은 \"어려운 문제를 더 어렵게 풀지만, 그 결과가 끝내주게 좋다\"는 것인데요.\n",
        "\n",
        "만약 너무 쉬운 문제에 딥러닝을 적용한다거나 DL이 아닌 ML 방법론들보다 더 많은 리소스를 하는데도 성능이 낮으면 안되겠죠?"
      ],
      "metadata": {
        "id": "TZae1dEFeqHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Chance Level 이 넘는지 확인**"
      ],
      "metadata": {
        "id": "f3CKaTrsesC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이진 분류(Binary Classification)는 Chance level이 0.5(50%)인 문제입니다. \n",
        "\n",
        "말 그대로 하나로 찍는 머신이 이라도 약 50%정도는 달성한다는 뜻인데요. (데이터가 편향된 경우는 제외)\n",
        "\n",
        "여러분이 선택한 문제가 MNIST와 같은 10개 클래스를 가진 다중 분류라면 Chance level이 0.1(10%)이 되겠죠.\n",
        "\n",
        "---\n",
        "\n",
        "여러분이 선택한 문제에 딥러닝을 적용한 결과가 Chance level 보다 월등하게 좋은 성능을 기록하는 지를 체크해봅니다.\n",
        "\n",
        "그렇지 않다면 데이터를 다시 들여다 보거나 모델을 다시 뜯어보면서 내 가설이 틀렸을 수 있다는 것을 확인하여 봅니다."
      ],
      "metadata": {
        "id": "8cxMA-qteupy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 모델 검증(Validation)**"
      ],
      "metadata": {
        "id": "u1Q-E1Ace4-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 만들어서 어느정도 성능이 나왔다면, CV을 통해서 일반화될 가능성이 있는 지 확인해봅니다.\n",
        "\n",
        "K-Fold 교차 검증을 통해 일반화가 어느정도 되는지 알 수 있습니다.\n",
        "\n",
        "더불어 하이퍼파라미터를 변경하면서 최적화까지 해 볼 수 있겠습니다.\n",
        "\n",
        "물론 해당 과정에서 너무 많은 시간이 소요되지 않도록 잘 조정해야 합니다."
      ],
      "metadata": {
        "id": "dwhOvwJre7pQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. (Option) Requirements.txt 제작 및 재구현**"
      ],
      "metadata": {
        "id": "VWpGYoOzfAId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which python\n",
        "!python --version\n",
        "!echo $PYTHONPATH # /env/python\n",
        "%env PYTHONPATH=  #pythonpath 초기화\n",
        "!which conda# should return /usr/local/bin/conda\n",
        "!conda --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f_h9pef3B9",
        "outputId": "7951e3bf-c24f-41cd-f1ac-c5967a53d0e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.7.10\n",
            "/env/python\n",
            "env: PYTHONPATH=#pythonpath 초기화\n",
            "/usr/local/bin/conda\n",
            "conda 4.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = \"\"\"\n",
        "gensim==3.8.1\n",
        "pyLDAvis==2.1.2\n",
        "spacy==2.2.3\n",
        "scikit-learn==0.24\n",
        "seaborn==0.11.0\n",
        "squarify==0.4.3\n",
        "tensorboard==2.4.0 \n",
        "tensorboard-plugin-wit==1.7.0 \n",
        "tensorflow==2.3.0 \n",
        "tensorflow-estimator==2.3.0\n",
        "keras==2.0.8\n",
        "ipykernel\n",
        "nltk\n",
        "pandas\n",
        "numpy\n",
        "scipy\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UQiBua7WgQXv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = \"\"\"text_file = open(\"requirements.txt\", \"w+\");text_file.write(p);text_file.close()\"\"\" \n",
        "\n",
        "exec(c)"
      ],
      "metadata": {
        "id": "lKiA5v_8gyrG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iBL7JKjXg4Ry",
        "outputId": "001bdc83-59b8-4223-856d-2a4019e0b3cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting pyLDAvis==2.1.2\n",
            "  Using cached pyLDAvis-2.1.2-py2.py3-none-any.whl\n",
            "Collecting spacy==2.2.3\n",
            "  Using cached spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "Collecting scikit-learn==0.24\n",
            "  Downloading scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting seaborn==0.11.0\n",
            "  Using cached seaborn-0.11.0-py3-none-any.whl (283 kB)\n",
            "Collecting squarify==0.4.3\n",
            "  Using cached squarify-0.4.3-py3-none-any.whl (4.3 kB)\n",
            "Collecting tensorboard==2.4.0\n",
            "  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
            "Collecting tensorboard-plugin-wit==1.7.0\n",
            "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
            "Collecting tensorflow==2.3.0\n",
            "  Using cached tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "Collecting tensorflow-estimator==2.3.0\n",
            "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "Collecting keras==2.0.8\n",
            "  Using cached Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.6.1-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting nltk\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 67.9 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 355 kB/s \n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/site-packages (from gensim==3.8.1->-r requirements.txt (line 2)) (1.15.0)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting numexpr\n",
            "  Downloading numexpr-2.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)\n",
            "\u001b[K     |████████████████████████████████| 379 kB 39.1 MB/s \n",
            "\u001b[?25hCollecting jinja2>=2.7.2\n",
            "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 3)) (0.36.2)\n",
            "Collecting funcy\n",
            "  Using cached funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Collecting joblib>=0.8.4\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 49.3 MB/s \n",
            "\u001b[?25hCollecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting pytest\n",
            "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 53.7 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting matplotlib>=2.2\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 51.9 MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.25.1)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
            "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (49.6.0.post20210108)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Using cached thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 58.2 MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 66.7 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.0\n",
            "  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 44.6 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.8\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting keras-preprocessing<1.2,>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 939 kB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 10.0 MB/s \n",
            "\u001b[?25hCollecting astunparse==1.6.3\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gast==0.3.3\n",
            "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting pytz>=2017.3\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 65.0 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata>=0.20\n",
            "  Downloading importlib_metadata-4.10.0-py3-none-any.whl (17 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting pyparsing>=2.2.1\n",
            "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pillow>=6.2.0\n",
            "  Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 49.7 MB/s \n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
            "\u001b[K     |████████████████████████████████| 890 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.3 MB/s \n",
            "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2020.12.5)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3->-r requirements.txt (line 4)) (4.59.0)\n",
            "Collecting argcomplete>=1.12.3\n",
            "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting jupyter-client<8.0\n",
            "  Downloading jupyter_client-7.1.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting debugpy<2.0,>=1.0.0\n",
            "  Downloading debugpy-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 56.9 MB/s \n",
            "\u001b[?25hCollecting ipython>=7.23.1\n",
            "  Downloading ipython-7.31.0-py3-none-any.whl (792 kB)\n",
            "\u001b[K     |████████████████████████████████| 792 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB)\n",
            "Collecting matplotlib-inline<0.2.0,>=0.1.0\n",
            "  Downloading matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
            "Collecting tornado<7.0,>=4.2\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 47.1 MB/s \n",
            "\u001b[?25hCollecting traitlets<6.0,>=5.1.0\n",
            "  Downloading traitlets-5.1.1-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting pexpect>4.3\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.24-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting pygments\n",
            "  Downloading Pygments-2.11.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.6 MB/s \n",
            "\u001b[?25hCollecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 49.5 MB/s \n",
            "\u001b[?25hCollecting parso<0.9.0,>=0.8.0\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting entrypoints\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Collecting jupyter-core>=4.6.0\n",
            "  Downloading jupyter_core-4.9.1-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting pyzmq>=13\n",
            "  Downloading pyzmq-22.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.6 MB/s \n",
            "\u001b[?25hCollecting ptyprocess>=0.5\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting wcwidth\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting click\n",
            "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting py>=1.8.2\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting attrs>=19.2.0\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: termcolor, future\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=7d1993ad7cffa97439d98e530103ce69b05243e51f6b824329ccc6b6546371e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=9d11850e2092d55caefcc22146fdff4ce0b87a25e4d7d78d20d130248a8543bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built termcolor future\n",
            "Installing collected packages: pyasn1, zipp, typing-extensions, rsa, pyasn1-modules, oauthlib, cachetools, wcwidth, traitlets, requests-oauthlib, pyparsing, ptyprocess, parso, numpy, murmurhash, importlib-metadata, google-auth, cymem, werkzeug, wasabi, tornado, toml, tensorboard-plugin-wit, srsly, pyzmq, pytz, python-dateutil, pygments, py, protobuf, prompt-toolkit, preshed, pluggy, plac, pillow, pickleshare, pexpect, packaging, nest-asyncio, matplotlib-inline, MarkupSafe, markdown, kiwisolver, jupyter-core, jedi, iniconfig, grpcio, google-auth-oauthlib, fonttools, entrypoints, decorator, cycler, blis, backcall, attrs, absl-py, wrapt, threadpoolctl, thinc, termcolor, tensorflow-estimator, tensorboard, smart-open, scipy, regex, pyyaml, pytest, pandas, opt-einsum, numexpr, matplotlib, keras-preprocessing, jupyter-client, joblib, jinja2, ipython, h5py, google-pasta, gast, future, funcy, debugpy, click, catalogue, astunparse, argcomplete, tensorflow, squarify, spacy, seaborn, scikit-learn, pyLDAvis, nltk, keras, ipykernel, gensim\n",
            "Successfully installed MarkupSafe-2.0.1 absl-py-1.0.0 argcomplete-2.0.0 astunparse-1.6.3 attrs-21.4.0 backcall-0.2.0 blis-0.4.1 cachetools-4.2.4 catalogue-1.0.0 click-8.0.3 cycler-0.11.0 cymem-2.0.6 debugpy-1.5.1 decorator-5.1.1 entrypoints-0.3 fonttools-4.28.5 funcy-1.17 future-0.18.2 gast-0.3.3 gensim-3.8.1 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.43.0 h5py-2.10.0 importlib-metadata-4.10.0 iniconfig-1.1.1 ipykernel-6.6.1 ipython-7.31.0 jedi-0.18.1 jinja2-3.0.3 joblib-1.1.0 jupyter-client-7.1.0 jupyter-core-4.9.1 keras-2.0.8 keras-preprocessing-1.1.2 kiwisolver-1.3.2 markdown-3.3.6 matplotlib-3.5.1 matplotlib-inline-0.1.3 murmurhash-1.0.6 nest-asyncio-1.5.4 nltk-3.6.7 numexpr-2.8.1 numpy-1.18.5 oauthlib-3.1.1 opt-einsum-3.3.0 packaging-21.3 pandas-1.3.5 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pillow-9.0.0 plac-1.1.3 pluggy-1.0.0 preshed-3.0.6 prompt-toolkit-3.0.24 protobuf-3.19.1 ptyprocess-0.7.0 py-1.11.0 pyLDAvis-2.1.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.11.2 pyparsing-3.0.6 pytest-6.2.5 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 pyzmq-22.3.0 regex-2021.11.10 requests-oauthlib-1.3.0 rsa-4.8 scikit-learn-0.24.0 scipy-1.4.1 seaborn-0.11.0 smart-open-5.2.1 spacy-2.2.3 squarify-0.4.3 srsly-1.0.5 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 thinc-7.3.1 threadpoolctl-3.0.0 toml-0.10.2 tornado-6.1 traitlets-5.1.1 typing-extensions-4.0.1 wasabi-0.9.0 wcwidth-0.2.5 werkzeug-2.0.2 wrapt-1.13.3 zipp-3.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "dateutil",
                  "google",
                  "jupyter_core",
                  "kiwisolver",
                  "numpy",
                  "pexpect",
                  "pickleshare",
                  "pyparsing",
                  "traitlets",
                  "wcwidth",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 콘다 설치 방법1\n",
        "# # Miniconda 설치\n",
        "# %%bash\n",
        "\n",
        "# MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "# MINICONDA_PREFIX=/usr/local\n",
        "# wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "# chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "# ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\n",
        "\n",
        "# import sys\n",
        "# sys.path\n",
        "\n",
        "# !ls /usr/local/lib/python3.8/dist-packages\n",
        "\n",
        "# import sys\n",
        "# _ = (sys.path.append(\"/usr/local/lib/python3.8/site-packages\"))\n",
        "\n",
        "# sys.path\n",
        "# !conda install --channel conda-forge featuretools --yes\n",
        "\n",
        "# !ls\n",
        "\n",
        "# !conda env create -f /content/drive/MyDrive/RogerHeederer/ChatBot/KoGPT2-personachat-master/environment.yml\n",
        "\n",
        "# %%shell\n",
        "# $conda activate cm\n",
        "\n",
        "# %%shell\n",
        "# conda info --envs"
      ],
      "metadata": {
        "id": "6kux6N4ywZUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 콘다 설치 방법2\n",
        "# # 가상환경 만들기\n",
        "# !conda create -n envname python==3.8\n",
        "\n",
        "# # 먼저 가상환경 실행시키고 가상환경 확인\n",
        "# !source activate envname && conda env list\n",
        "\n",
        "# # 설치파일이 있다면 설치하기\n",
        "# !conda install --channel conda-forge --file requirements.txt --yes"
      ],
      "metadata": {
        "id": "A1BrHLsiwdRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 콘다 설치 방법3\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "# !pip install -q condacolab\n",
        "\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "\n",
        "# import sys\n",
        "# sys.path\n",
        "\n",
        "# %%shell\n",
        "# python --version\n",
        "\n",
        "# import condacolab\n",
        "# condacolab.check()"
      ],
      "metadata": {
        "id": "mkThAaD_w0i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze > requirements.txt\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "JMQj5PJBOoW7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "언제까지 Colab만 쓰지는 않을 것입니다.\n",
        "\n",
        "여러분이 만든 딥러닝 모델을 다시 사용할 수 있도록 저장하고 새로운 환경에서도 바로 동작할 수 있도록 Requirements.txt를 만들어봅니다.\n",
        "\n",
        "만든 Requirements.txt 를 이용하여 가상환경 혹은 독립된 PC에서 같은 프로젝트를 진행하여 봅니다."
      ],
      "metadata": {
        "id": "vclxy9yBe-Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **제출물**\n",
        "\n",
        "아래 발표물을 압축하여 AI_(기수)_(이름).zip 형태로 제출합니다. Ex) AI_00_홍길동.zip\n",
        "\n",
        "발표 영상 (5분 이상 10분 이내로 작성합니다\u001d.)\n",
        "코드가 포함된 .ipynb 파일\n",
        "(제작하였을 경우) PPT 혹은 Keynote 파일\n"
      ],
      "metadata": {
        "id": "ETsbDcjEfFZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **프로젝트 채점 기준**"
      ],
      "metadata": {
        "id": "_cFICRbtfK8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. \"여러분이 풀고자 하는 문제를 잘 설정하였는가?\"**\n",
        "\n",
        "여러분이 선택한 데이터셋을 사용하여 어떤 문제를 풀고자 하는 지를 영상에서 잘 설명해주세요.\n",
        "\n",
        "\n",
        "단순히 익숙하다는 이유로 MNIST, cifar100 데이터를 사용하면 안되겠죠?\n",
        "\n",
        "\n",
        "여러분만의 문제와 데이터셋을 마련하는 것이 이번 프로젝트의 첫 번째 목표입니다!"
      ],
      "metadata": {
        "id": "NWE_KIWdfOIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. \"문제를 풀기 위한 모델 선택을 알맞게 설정하였는가?\"**\n",
        "\n",
        "\n",
        "여러분이 선택한 모델에 대한 구조를 설명할 필요는 없으며 해당 모델을 왜 선택하였는 지에 대해서 설명해주세요.\n",
        "\n",
        "\n",
        "예를 들어, 이미지 분류 문제에 단순한 LSTM을 적용하면 안 될 겁니다.\n",
        "\n",
        "\n",
        "풀고자 하는 문제와 알맞는 모델을 잘 선택하는 것이 이번 프로젝트의 두 번째 목표입니다!\n"
      ],
      "metadata": {
        "id": "Vo--a6PnfYol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. \"모델 학습을 제대로 진행하였는가?\"**\n",
        "\n",
        "\n",
        "데이터를 어떻게 전처리하였고 모델에 입력하였는 지를 설명해주세요.\n",
        "\n",
        "\n",
        "모델의 성능이 좋아야 할 필요는 없겠지만 모델 학습은 진행되어야 합니다."
      ],
      "metadata": {
        "id": "ZekcM_tEffF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 완성하지 못한 부분에 대하여 \"한계점과 추후 해결 방안을 알맞게 작성하였는가?\"**\n",
        "\n",
        "\n",
        "프로젝트 기간 동안 제시한 문제에 대해서 만족할 만한 성능을 얻지 못할 수도 있습니다.\n",
        "\n",
        "\n",
        "여러분이 만족할 만한 결과를 위해서 어떤 방향으로 프로젝트를 발전시켜 나가야 할지 구체적으로 설명해주세요."
      ],
      "metadata": {
        "id": "weLV9CZ_fkEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 영상 길이는 5분 이상 10분 이내로 작성합니다.**\n",
        "\n",
        " 너무 길어지거나 너무 짧아지지 않도록 해주세요!"
      ],
      "metadata": {
        "id": "I3b9s5oJfo1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NqRaazydfKqb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4RGKhpFkV8bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l1oxpjlkV3cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23C2WmP6Vaap"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}